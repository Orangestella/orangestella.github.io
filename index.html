<!DOCTYPE html>
<html>
    <head>
        <title>Guanyu Hou</title>
        <style>
            body {
                font-family: Arial, sans-serif;
                margin: 0 auto; /* 居中对齐 */
                padding: 1.25rem; /* 留出1.25rem的间隙 */
                max-width: 50rem; /* 最大宽度为50rem */
                background-color: #f4f4f4;
            }
        
            h1, h3 {
                color: #333;
            }
        
            #profile {
                width: 12.5rem;
                height: 12.5rem;
            }

            #papers li {
                display: flex;
                align-items: center;
                margin: 0.625rem 0;
            }

            #papers img {
                width: 12.25rem;
                height: auto; /* 按原比例缩放 */
                cursor: pointer; /* 鼠标悬停时变为手形 */
                transition: transform 0.4s ease; /* 添加过渡效果 */
            }

            #papers img:hover {
                transform: scale(3); /* 鼠标悬停时放大 */
            }
        
            .vertical {
                display: flex;
                flex-direction: column;
                list-style-type: none;
                padding: 0;
            }
        
            footer {
                text-align: center;
                padding: 1.25rem;
                background-color: #333;
                color: #fff;
            }

            .meta_data{
                color: gray;
            }
        
            /* 媒体查询，当屏幕宽度小于37.5rem时，应用以下样式 */
            @media (max-width: 37.5rem) {
                body {
                    padding: 0.625rem; /* 在小屏幕上留出0.625rem的间隙 */
                }
        
                #profile {
                    width: 6.25rem;
                    height: 6.25rem;
                }
        
                #papers img {
                    width: 1.5625rem;
                    height: 1.5625rem;
                }
            }
        </style>
        <link rel="icon" href="https://img.icons8.com/external-smashingstocks-glyph-smashing-stocks/66/external-scholar-online-courses-learning-and-knowledge-smashingstocks-glyph-smashing-stocks.png" type="image/png">
        
    </head>
    <body>
        <!-- <img src="img\profile.png" alt="profile" id="profile"> -->
        <h1>Guanyu Hou</h1>
        <span><a href="mailto:guanyu.hou@student.manchester.ac.uk">E-mail</a>&nbsp;/</li>
        <span><a href="res\cv_en.pdf">CV (English)</a> &nbsp;/</li>
        <span><a href="res\cv_cn.pdf">CV (中文)</a> &nbsp;/</li>
        <span><a href="https://github.com/Orangestella">GitHub</a>&nbsp;/</li>
        <span><a href="https://scholar.google.com/citations?user=6rsZHFkAAAAJ">Google Scholar</a></li><br>
        <p id="bio">Hello, I'm Guanyu Hou. I am currently a Master of Science in Artificial Intelligence student at The University of Manchester. My research focuses on AI safety and security. To date, I have 7 publications , with work accepted at top-tier conferences including AAAI (Oral, top 5%) , EMNLP , and ICASSP. I also serve as a reviewer for AAAI 2026  and am eager to continue advancing research in this field.</p>
        <hr>
        <h3>Research</h3>
        <p>I am interested in the field of trustworthy and safe AI. I have participated in research related to the security of large models. My experience includes work on backdoor attacks in LLMs and multi-modal systems, privacy in Retrieval-Augmented Generation (RAG), and the robustness of large audio language models. This research has been a valuable learning opportunity, and I am eager to continue exploring this field.</p>
        <ul id="papers" class="vertical">
            <li>
                <br>
                <img src="img/emnlp.jpg" alt="emnlp">
                <div>
                <b>Evaluating Robustness of Large Audio Language Models to Audio Injection: An Empirical Study</b>
                <div><i class="meta_data">The 2025 Conference on Empirical Methods in Natural Language Processing</i></div>
                <div class="meta_data"><b>Guanyu Hou</b>, Jiaming He, Yinhang Zhou, Ji Guo, Yitong Qiao, Rui Zhang, Wenbo Jiang</div>
                <div>Large Audio-Language Models (LALMs) are increasingly deployed in real-world applications, yet their robustness against malicious audio injection attacks remains underexplored. This study systematically evaluates five leading LALMs across four attack scenarios...... </div>
                <a href="https://arxiv.org/pdf/2505.19598?">Click to read more</a>
                </div>
            </li>
            <li>
                <br>
                <img src="img/mmaisa.jpeg" alt="mmasia">
                <div>
                <b>When Hallucinated Concepts Cross Modals: Unveiling Backdoor Vulnerability in Multi-modal In-context Learning</b>
                <div><i class="meta_data">ACM Multimedia Asia 2025</i></div>
                <div class="meta_data"><b>Guanyu Hou</b>, Jiaming He, Yitong Qiao, Jiachen Li, Zihan Wang, Qiyang Song, Hongwei Li, Wenbo Jiang</div>
                <div> Due to the remarkable performance of multi-modal large language models (MLLMs)inmulti-modalcapabilities, multi-modal in-context learning (M-ICL) has garnered widespread attention for fast adapting MLLMs to downstream tasks. However, the vulnerability of M-ICL to attacks remains largely unexplored....... </div>
                <!-- <a href="https://arxiv.org/pdf/2505.19598?">Click to read more</a> -->
                </div>
            </li>
            <li>
                <br>
                <img src="img\aaai.png" alt="Watch Out for Your Guidance on Generation!">
                <div>
                <b>Watch Out for Your Guidance on Generation! Exploring Conditional Backdoor Attacks against Large Language Models</b>
                <div><i class="meta_data">The 39th Annual AAAI Conference on Artificial Intelligence</i></div>
                <div class="meta_data">Jiaming He, Wenbo Jiang, <b>Guanyu Hou</b>, Wenshu Fan, Rui Zhang, Hongwei Li</div>
                <div>To enhance the stealthiness of backdoor activation, we present a new poisoning paradigm against LLMs triggered by specifying generation conditions, which are commonly adopted strategies by users during model inference. The poisoned model performs normally for output under normal/other generation conditions, while becomes harmful for output under target generation conditions. To achieve this objective, we introduce BrieFool, an efficient attack framework......</div>
                <a href="https://ojs.aaai.org/index.php/AAAI/article/view/34819">Click to read more</a>
                </div>
            </li>
            <li>
                <br>
                <img src="img\icassp.jpg" alt="PREES">
                <div>
                <b>PRESS: Defending Privacy in Retrieval-Augmented Generation via Embedding Space Shifting</b>
                <div><i class="meta_data">2025 IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP 2025)</i></div>
                <div class="meta_data">Jiaming He, Cheng Liu, <b>Guanyu Hou</b>, Wenbo Jiang, Jiachen Li</div>
                <div>Retrieval-augmented generation (RAG) systems are exposed to substantial privacy risks during the information retrieval process, leading to potential data leakage of private information. In this work, we present a Privacy-preserving Retrieval-augmented generation via Embedding Space Shifting (PRESS), systematically exploring how to protect privacy in RAG systems...... </div>
                <a href="https://ieeexplore.ieee.org/document/10887843">Click to read more</a>
                </div>
            </li>
            <li>
                <br>
                <img src="img\icme.jpg" alt="ICME">
                <div>
                <b>Weaponizing Tokens: Backdooring Text-to-Image Generation via Token Remapping</b>
                <div><i class="meta_data">IEEE International Conference on Multimedia&Expo 2025</i></div>
                <div class="meta_data">Jiaming He, Wenbo Jiang, <b>Guanyu Hou</b>, Qiyang Song, Guo Ji and Hongwei Li</div>
                <div>In this work, we first investigate the backdoor attack against Text-to-image generation by manipulating text tokenizer. Our backdoor attack exploits the semantic conditioning role of text tokenizer in the text-to-image generation. We propose an Automatized Remapping Framework with Optimized Tokens (AROT) for finding the best target tokens to remap the trigger token in the mapping space...... </div>
                <!-- <a href="https://ieeexplore.ieee.org/document/10887843">Click to read more</a> -->
                </div>
            </li>
            <li>
                <br>
                <img src="img\electronics-13-02858-g001.webp" alt="data_stealing">
                <div>
                <b>Data Stealing Attacks against Large Language Models via Backdooring</b>
                <div><i class="meta_data">Electronics 13 (14), 2858</i></div>
                <div class="meta_data">Jiaming He, <b>Guanyu Hou</b>, Xinyue Jia, Yangyang Chen, Wenqi Liao, Yinhang Zhou, Rang Zhou</div>
                <div>Large language models (LLMs) have gained immense attention and are being increasingly applied in various domains. However, this technological leap forward poses serious security and privacy concerns. This paper explores a novel approach to data stealing attacks by introducing an adaptive method to extract private training data from pre-trained LLMs via backdooring......</div>
                <a href="https://www.mdpi.com/2079-9292/13/14/2858">Click to read more</a>
                </div>
            </li>
            <li>
                <br>
                <img src="img\icsp.jpg" alt="element_injection">
                <div>
                <b>Embedding Based Sensitive Element Injection against Text-to-Image Generative Models</b>
                <div><i class="meta_data">2024 9th International Conference on Intelligent Computing and Signal Processing (ICSP)</i></div>
                <div class="meta_data">Benrui Jiang, Kan Chen, <b>Guanyu Hou</b>, Xiying Chen, Jiaming He</div>
                <div>Text-to-image technique is becoming increasingly popular among researchers and the public. Unfortunately, we found that text-to-image technique has certain security issues. In our work, we explore a novel attack paradigm for the text-to-image scenarios. By our attack, we will use target embeddings to manipulate the user embeddings to generate malicious images...... </div>
                <a href="https://ieeexplore.ieee.org/document/10743442">Click to read more</a>
                </div>
            </li>
        </ul>
        <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=qyy9M7Z56cizovL7_uab9oWhsHor6y5BKyxW5Al-8qY&cl=ffffff&w=a"></script>
        <footer>
            <small>©Guanyu Hou</small>
        </footer>
    </body>
</html>