<!DOCTYPE html>
<html>
    <head>
        <title>Guanyu Hou</title>
        <style>
            body {
                font-family: Arial, sans-serif;
                margin: 0 auto; /* 居中对齐 */
                padding: 1.25rem; /* 留出1.25rem的间隙 */
                max-width: 50rem; /* 最大宽度为50rem */
                background-color: #f4f4f4;
            }
        
            h1, h3 {
                color: #333;
            }
        
            #profile {
                width: 12.5rem;
                height: 12.5rem;
            }

            #papers li {
                display: flex;
                align-items: center;
                margin: 0.625rem 0;
            }

            #papers img {
                width: 12.25rem;
                height: auto; /* 按原比例缩放 */
                cursor: pointer; /* 鼠标悬停时变为手形 */
                transition: transform 0.4s ease; /* 添加过渡效果 */
            }

            #papers img:hover {
                transform: scale(3); /* 鼠标悬停时放大 */
            }
        
            .vertical {
                display: flex;
                flex-direction: column;
                list-style-type: none;
                padding: 0;
            }
        
            footer {
                text-align: center;
                padding: 1.25rem;
                background-color: #333;
                color: #fff;
            }

            .meta_data{
                color: gray;
            }
        
            /* 媒体查询，当屏幕宽度小于37.5rem时，应用以下样式 */
            @media (max-width: 37.5rem) {
                body {
                    padding: 0.625rem; /* 在小屏幕上留出0.625rem的间隙 */
                }
        
                #profile {
                    width: 6.25rem;
                    height: 6.25rem;
                }
        
                #papers img {
                    width: 1.5625rem;
                    height: 1.5625rem;
                }
            }
        </style>
        <link rel="icon" href="https://img.icons8.com/external-smashingstocks-glyph-smashing-stocks/66/external-scholar-online-courses-learning-and-knowledge-smashingstocks-glyph-smashing-stocks.png" type="image/png">
        
    </head>
    <body>
        <img src="img\profile.png" alt="profile" id="profile">
        <h1>Guanyu Hou</h1>
        <span><a href="mailto:edgarhou03@gmail.com">E-mail</a>&nbsp;/</li>
        <span><a href="res\cv.pdf">CV</a> &nbsp;/</li>
        <span><a href="https://github.com/Orangestella">GitHub</a></li><br>
        <p id="bio">Hello, I'm Guanyu Hou, an undergraduate student majoring in Software Engineering at Oxford Brookes College, Chengdu University of Technology and expected to graduate in July 2025. I have been involved in AI-related research since 2024. So far, I have completed three works in the field of AI security. In the future, I hope to be exposed to more research in the field of AI security, including computer vision security, language model security, etc.</p>
        <hr>
        <h3>Research</h3>
        <p>I am interested in the field of trustworthy and safe AI. Under the guidance of various scholars, including Prof. <a href="https://cist.cdut.edu.cn/info/1118/4312.htm">Rang Zhou</a>, I have participated in some research on backdoor attacks on large language models, and element injection attacks on Text-to-image models, and have gained some achievements and experience.</p>
        <ul id="papers" class="vertical">
            <li>
                <br>
                <img src="img\electronics-13-02858-g001.webp" alt="data_stealing">
                <div>
                <b>Data Stealing Attacks against Large Language Models via Backdooring</b>
                <div><i class="meta_data">Electronics 13 (14), 2858</i></div>
                <div class="meta_data">Jiaming He, <b>Guanyu Hou</b>, Xinyue Jia, Yangyang Chen, Wenqi Liao, Yinhang Zhou, Rang Zhou</div>
                <div>Large language models (LLMs) have gained immense attention and are being increasingly applied in various domains. However, this technological leap forward poses serious security and privacy concerns. This paper explores a novel approach to data stealing attacks by introducing an adaptive method to extract private training data from pre-trained LLMs via backdooring......</div>
                <a href="https://www.mdpi.com/2079-9292/13/14/2858">Click to read more</a>
                </div>
            </li>
            <li>
                <br>
                <img src="img\icsp.jpg" alt="element_injection">
                <div>
                <b>Embedding Based Sensitive Element Injection against Text-to-Image Generative Models</b>
                <div><i class="meta_data">2024 9th International Conference on Intelligent Computing and Signal Processing (ICSP)</i></div>
                <div class="meta_data">Benrui Jiang, Kan Chen, <b>Guanyu Hou</b>, Xiying Chen, Jiaming He</div>
                <div>Text-to-image technique is becoming increasingly popular among researchers and the public. Unfortunately, we found that text-to-image technique has certain security issues. In our work, we explore a novel attack paradigm for the text-to-image scenarios. By our attack, we will use target embeddings to manipulate the user embeddings to generate malicious images..... </div>
                <a href="res/ICSP.pdf">Click to read more</a>
                </div>
            </li>
            <li>
                <br>
                <img src="img\talk_too_much.jpg" alt="talk_too_much">
                <div>
                <b>Talk Too Much: Poisoning Large Language Models under Token Limit</b>
                <div class="meta_data">Jiaming He, Wenbo Jiang, <b>Guanyu Hou</b>, Wenshu Fan, Rui Zhang, Hongwei Li</div>
                <div>Mainstream poisoning attacks on large language models (LLMs) typically set a fixed trigger in the input instance and specific responses for triggered queries. However, the fixed trigger setting may be easily detected by human detection, limiting the effectiveness and practicality in real-world scenarios. To enhance the stealthiness of the trigger, we present a poisoning attack against LLMs that is triggered by a generation/output condition-token limitation.....</div>
                <a href="https://arxiv.org/abs/2404.14795">Click to read more</a>
                </div>
            </li>
        </ul>
        <footer>
            <small>©Guanyu Hou</small>
        </footer>
    </body>
</html>