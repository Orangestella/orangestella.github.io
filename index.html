<!DOCTYPE html>
<html>
    <head>
        <title>Guanyu Hou</title>
        <style>
            body {
                font-family: Arial, sans-serif;
                margin: 0 auto; /* 居中对齐 */
                padding: 1.25rem; /* 留出1.25rem的间隙 */
                max-width: 50rem; /* 最大宽度为50rem */
                background-color: #f4f4f4;
            }
        
            h1, h3 {
                color: #333;
            }
        
            #profile {
                width: 12.5rem;
                height: 12.5rem;
            }

            #papers li {
                display: flex;
                align-items: center;
                margin: 0.625rem 0;
            }

            #papers img {
                width: 12.25rem;
                height: auto; /* 按原比例缩放 */
                cursor: pointer; /* 鼠标悬停时变为手形 */
                transition: transform 0.4s ease; /* 添加过渡效果 */
            }

            #papers img:hover {
                transform: scale(3); /* 鼠标悬停时放大 */
            }
        
            .vertical {
                display: flex;
                flex-direction: column;
                list-style-type: none;
                padding: 0;
            }
        
            footer {
                text-align: center;
                padding: 1.25rem;
                background-color: #333;
                color: #fff;
            }

            .meta_data{
                color: gray;
            }
        
            /* 媒体查询，当屏幕宽度小于37.5rem时，应用以下样式 */
            @media (max-width: 37.5rem) {
                body {
                    padding: 0.625rem; /* 在小屏幕上留出0.625rem的间隙 */
                }
        
                #profile {
                    width: 6.25rem;
                    height: 6.25rem;
                }
        
                #papers img {
                    width: 1.5625rem;
                    height: 1.5625rem;
                }
            }
        </style>
        <link rel="icon" href="https://img.icons8.com/external-smashingstocks-glyph-smashing-stocks/66/external-scholar-online-courses-learning-and-knowledge-smashingstocks-glyph-smashing-stocks.png" type="image/png">
        
    </head>
    <body>
        <img src="img\profile.png" alt="profile" id="profile">
        <h1>Guanyu Hou</h1>
        <span><a href="mailto:hou.guanyu@student.zy.cdut.edu.cn">E-mail</a>&nbsp;/</li>
        <span><a href="res\cv.pdf">CV</a> &nbsp;/</li>
        <span><a href="https://github.com/Orangestella">GitHub</a>&nbsp;/</li>
        <span><a href="https://scholar.google.com/citations?user=6rsZHFkAAAAJ">Google Scholar</a></li><br>
        <p id="bio">Hello, I'm Guanyu Hou, an undergraduate student majoring in Software Engineering at Oxford Brookes College, Chengdu University of Technology and expected to graduate in July 2025. I have been involved in AI-related research since 2024. So far, I have completed three works in the field of AI safety and security. In the future, I hope to be exposed to more research in this field.</p>
        <hr>
        <h3>Research</h3>
        <p>I am interested in the field of trustworthy and safe AI. Under the guidance of various scholars, including <a href="https://cist.cdut.edu.cn/info/1118/4312.htm">Dr. Rang Zhou</a>, I have participated in some research on backdoor attacks on large language models, and element injection attacks on Text-to-image models, and have gained some achievements and experience.</p>
        <ul id="papers" class="vertical">
            <li>
                <br>
                <img src="img\aaai.png" alt="Watch Out for Your Guidance on Generation!">
                <div>
                <b>Watch Out for Your Guidance on Generation! Exploring Conditional Backdoor Attacks against Large Language Models</b>
                <div><i class="meta_data">The 39th Annual AAAI Conference on Artificial Intelligence</i></div>
                <div class="meta_data">Jiaming He, Wenbo Jiang, <b>Guanyu Hou</b>, Wenshu Fan, Rui Zhang, Hongwei Li</div>
                <div>To enhance the stealthiness of backdoor activation, we present a new poisoning paradigm against LLMs triggered by specifying generation conditions, which are commonly adopted strategies by users during model inference. The poisoned model performs normally for output under normal/other generation conditions, while becomes harmful for output under target generation conditions. To achieve this objective, we introduce BrieFool, an efficient attack framework......</div>
                <a href="https://arxiv.org/abs/2404.14795">Click to read more</a>
                </div>
            </li>
            <li>
                <br>
                <img src="img\icassp.jpg" alt="PREES">
                <div>
                <b>PRESS: Defending Privacy in Retrieval-Augmented Generation via Embedding Space Shifting</b>
                <div><i class="meta_data">2025 IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP 2025)</i></div>
                <div class="meta_data">Jiaming He, Cheng Liu, <b>Guanyu Hou</b>, Wenbo Jiang, Jiachen Li</div>
                <div>Retrieval-augmented generation (RAG) systems are exposed to substantial privacy risks during the information retrieval process, leading to potential data leakage of private information. In this work, we present a Privacy-preserving Retrieval-augmented generation via Embedding Space Shifting (PRESS), systematically exploring how to protect privacy in RAG systems...... </div>
                </div>
            </li>
            <li>
                <br>
                <img src="img\electronics-13-02858-g001.webp" alt="data_stealing">
                <div>
                <b>Data Stealing Attacks against Large Language Models via Backdooring</b>
                <div><i class="meta_data">Electronics 13 (14), 2858</i></div>
                <div class="meta_data">Jiaming He, <b>Guanyu Hou</b>, Xinyue Jia, Yangyang Chen, Wenqi Liao, Yinhang Zhou, Rang Zhou</div>
                <div>Large language models (LLMs) have gained immense attention and are being increasingly applied in various domains. However, this technological leap forward poses serious security and privacy concerns. This paper explores a novel approach to data stealing attacks by introducing an adaptive method to extract private training data from pre-trained LLMs via backdooring......</div>
                <a href="https://www.mdpi.com/2079-9292/13/14/2858">Click to read more</a>
                </div>
            </li>
            <li>
                <br>
                <img src="img\icsp.jpg" alt="element_injection">
                <div>
                <b>Embedding Based Sensitive Element Injection against Text-to-Image Generative Models</b>
                <div><i class="meta_data">2024 9th International Conference on Intelligent Computing and Signal Processing (ICSP)</i></div>
                <div class="meta_data">Benrui Jiang, Kan Chen, <b>Guanyu Hou</b>, Xiying Chen, Jiaming He</div>
                <div>Text-to-image technique is becoming increasingly popular among researchers and the public. Unfortunately, we found that text-to-image technique has certain security issues. In our work, we explore a novel attack paradigm for the text-to-image scenarios. By our attack, we will use target embeddings to manipulate the user embeddings to generate malicious images...... </div>
                <a href="https://ieeexplore.ieee.org/document/10743442">Click to read more</a>
                </div>
            </li>
        </ul>
        <script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=IKA4q4qTUQ0QVDMMEwt79Cw3GJA6YOx87_AntB2oWjk"></script>
        <footer>
            <small>©Guanyu Hou</small>
        </footer>
    </body>
</html>